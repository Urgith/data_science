{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word__            __Stem__            \n",
      "programming         program             \n",
      "programs            program             \n",
      "programer           program             \n",
      "program             program             \n",
      "programmed          program             \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = {'program', 'programming', 'programer', 'programs', 'programmed'}\n",
    "\n",
    "print('{0:20}{1:20}'.format('--Word__', '__Stem__'))\n",
    "for word in example_words:\n",
    "    print ('{0:20}{1:20}'.format(word, ps.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Stem--            \n",
      "Python              python              \n",
      "programmers         programm            \n",
      "often               often               \n",
      "tend                tend                \n",
      "like                like                \n",
      "programming         program             \n",
      "is                  is                  \n",
      "python              python              \n",
      "because             becaus              \n",
      "its                 it                  \n",
      "like                like                \n",
      "english             english             \n",
      "We                  we                  \n",
      "call                call                \n",
      "people              peopl               \n",
      "who                 who                 \n",
      "program             program             \n",
      "in                  in                  \n",
      "python              python              \n",
      "pytonistas          pytonista           \n"
     ]
    }
   ],
   "source": [
    "example_sentence = 'Python programmers often tend like programming is python because it\\'s like english. We call people who program in python pytonistas.'\n",
    "example_sentence_no_punct = example_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "word_tokens = word_tokenize(example_sentence_no_punct)\n",
    "\n",
    "print('{0:20}{1:20}'.format('--Word--', '--Stem--'))\n",
    "for word in word_tokens:\n",
    "    print('{0:20}{1:20}'.format(word, ps.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "program             program             \n",
      "programming         program             \n",
      "programer           programer           \n",
      "programs            program             \n",
      "programmed          program             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading owm-1.4: Package 'owm-1.4' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('owm-1.4')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "example_words = ['program', 'programming', 'programer', 'programs', 'programmed']\n",
    "\n",
    "print('{0:20}{1:20}'.format('--Word--', '--Lemma--'))\n",
    "for word in example_words:\n",
    "    print('{0:20}{1:20}'.format(word, wnl.lemmatize(word, pos='v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Word--            --Lemma--           \n",
      "Python              Python              \n",
      "programmers         programmers         \n",
      "often               often               \n",
      "tend                tend                \n",
      "like                like                \n",
      "programming         program             \n",
      "because             because             \n",
      "its                 its                 \n",
      "like                like                \n",
      "english             english             \n",
      "We                  We                  \n",
      "call                call                \n",
      "people              people              \n",
      "who                 who                 \n",
      "program             program             \n",
      "in                  in                  \n",
      "python              python              \n",
      "pythonistas         pythonistas         \n"
     ]
    }
   ],
   "source": [
    "example_sentence = 'Python programmers often tend like programming because it\\'s like english. We call people who program in python pythonistas.'\n",
    "example_sentence_no_punct = example_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "word_tokens = word_tokenize(example_sentence_no_punct)\n",
    "\n",
    "print('{0:20}{1:20}'.format('--Word--', '--Lemma--'))\n",
    "for word in word_tokens:\n",
    "    print('{0:20}{1:20}'.format(word, wnl.lemmatize(word, pos='v')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
